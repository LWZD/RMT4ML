
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Section 4.5.3 Kernel ridge regression</title><meta name="generator" content="MATLAB 9.8"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2021-08-02"><meta name="DC.source" content="kernel_ridge.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Section 4.5.3 Kernel ridge regression</h1><!--introduction--><p>This page contains simulations in Section 4.5.3.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Classification of two mixture with same mean and covariance trace</a></li><li><a href="#2">Normal approximation of soft decision function <img src="kernel_ridge_eq11695215030516304895.png" alt="$g(x)$" style="width:10px;height:6px;"></a></li><li><a href="#3">Misclassification of real data as a function of decision threshold <img src="kernel_ridge_eq07512430879093679749.png" alt="$\xi$" style="width:3px;height:6px;"></a></li></ul></div><h2 id="1">Classification of two mixture with same mean and covariance trace</h2><pre class="codeinput">close <span class="string">all</span>; clear; clc

cs = [1/2 1/2];
k = length(cs); <span class="comment">% number of classes</span>
p = 512;
n = 2048;
n_test = 512;

means = @(i) zeros(p,1);
covs = @(i) toeplitz((4*(i-1)/10).^(0:(p-1)));

Delta_means = means(1)-means(2);
Delta_covs = covs(1) - covs(2);

gamma = 1;
tau = 2*trace(cs(1)*covs(1) + cs(2)*covs(2))/p;

fp_tau_loop = -3:.5:3;
store_error = zeros(length(fp_tau_loop),3);

iter = 1;
<span class="keyword">for</span> fp_tau = fp_tau_loop

    derivs=[4 fp_tau 2];
    coeffs=zeros(1,length(derivs));
    <span class="keyword">for</span> i=1:length(derivs)
        coeffs(i)=derivs(length(derivs)+1-i)/factorial(length(derivs)-i);
    <span class="keyword">end</span>
    f = @(x) polyval(coeffs,x-tau);

    nb_average_loop = 30;
    store_output = zeros(nb_average_loop,1);
    <span class="keyword">for</span>  average_index = 1:nb_average_loop
        W=zeros(p,n);
        W_test=zeros(p,n_test);
        <span class="keyword">for</span> i=1:k
            W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=covs(i)^(1/2)*randn(p,cs(i)*n);
            W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=covs(i)^(1/2)*randn(p,cs(i)*n_test);
        <span class="keyword">end</span>

        X=zeros(p,n);
        X_test=zeros(p,n_test);
        <span class="keyword">for</span> i=1:k
            X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)+means(i)*ones(1,cs(i)*n);
            X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)+means(i)*ones(1,cs(i)*n_test);
        <span class="keyword">end</span>
        XX = X'*X/p;

        K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');

        y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
        y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];

        inv_Q = K+n/gamma*eye(n);
        Q_y = inv_Q\y;
        Q_1 = inv_Q\ones(n,1);

        b = sum(Q_y)/sum(Q_1);
        alpha = Q_y-Q_1*b;

        g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;

        g_test = g(X_test)';
        store_output(average_index) = 1 - 1/n_test*(sum(g_test(1:cs(1)*n_test)&lt;0)+sum(g_test(cs(1)*n_test+1:end)&gt;0 ));
    <span class="keyword">end</span>

    <span class="comment">% theory</span>
    D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace(Delta_covs*Delta_covs) )/p;
    E = @(a) cs(2) - cs(1) + 2*(-1)^a*(1-cs(a))*gamma*cs(1)*cs(2)*D/p;
    V = @(a) 8*gamma^2*cs(1)^2*cs(2)^2*( derivs(3)^2*(trace(Delta_covs))^2*trace(covs(a)*covs(a))/p/p + 2*derivs(2)^2*( Delta_means'*covs(a)*Delta_means + trace( covs(a)*( covs(1)/cs(1) + covs(2)/cs(2) ) )/n ) )/p/p;

    store_error(iter,:) = [mean(store_output), std(store_output), cs(1)*qfunc(-E(1)/sqrt(V(1))) + cs(2)*qfunc(E(2)/sqrt(V(2)) )];
    iter = iter + 1;
<span class="keyword">end</span>

figure
hold <span class="string">on</span>
errorbar(fp_tau_loop, store_error(:,1), store_error(:,2))
plot(fp_tau_loop, store_error(:,3), <span class="string">'r'</span>)
xlabel( <span class="string">'$f^\prime(\tau_p)$'</span>, <span class="string">'Interpreter'</span>, <span class="string">'latex'</span>)
ylabel(<span class="string">'Misclassification rate'</span>, <span class="string">'Interpreter'</span>, <span class="string">'latex'</span>)
</pre><img vspace="5" hspace="5" src="kernel_ridge_01.png" alt=""> <h2 id="2">Normal approximation of soft decision function <img src="kernel_ridge_eq11695215030516304895.png" alt="$g(x)$" style="width:10px;height:6px;"></h2><pre class="codeinput">close <span class="string">all</span>; clear; clc

cs = [1/2 1/2];
k = length(cs); <span class="comment">% nb of classes</span>
n = 2048;
n_test = 512;

gamma = 1; <span class="comment">% regularization</span>
f = @(t) exp(-t/2); <span class="comment">% RBF kernel</span>

data_choice = <span class="string">'MNIST'</span>; <span class="comment">% 'MNIST', 'fashion',</span>

<span class="keyword">switch</span> data_choice
    <span class="keyword">case</span> <span class="string">'MNIST'</span>
        selected_labels=[7 9]; <span class="comment">% mean [0 1], [5 6]</span>
        init_data = loadMNISTImages(<span class="string">'../datasets/MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'fashion'</span>
        selected_labels=[8 9];
        init_data = loadMNISTImages(<span class="string">'../datasets/fashion-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/fashion-MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'Kuzushiji'</span>
        selected_labels=[3 4];
        init_data = loadMNISTImages(<span class="string">'../datasets/Kuzushiji-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/Kuzushiji-MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'kannada'</span>
        selected_labels=[4 8];
        init_data = loadMNISTImages(<span class="string">'../datasets/kannada-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/kannada-MNIST/train-labels-idx1-ubyte'</span>);
<span class="keyword">end</span>


[labels,idx_init_labels]=sort(init_labels,<span class="string">'ascend'</span>);
images=init_data(:,idx_init_labels);
init_n=length(images(1,:));

p=length(images(:,1));

mean_images=mean(images,2);
norm2_images=0;
<span class="keyword">for</span> i=1:init_n
    norm2_images=norm2_images+1/init_n*norm(images(:,i)-mean_images)^2;
<span class="keyword">end</span>
images=(images-mean_images*ones(1,size(images,2)))/sqrt(norm2_images)*sqrt(p);


selected_images=[];
MNIST = cell(length(selected_labels),1);
j=1;
<span class="keyword">for</span> i=selected_labels
    selected_images=[selected_images images(:,labels==i)];
    MNIST{j}=images(:,labels==i);
    j=j+1;
<span class="keyword">end</span>

mean_selected_images=mean(selected_images,2);
norm2_selected_images=mean(sum(abs(selected_images-mean_selected_images*ones(1,length(selected_images))).^2));

<span class="keyword">for</span> j=1:length(selected_labels)
    MNIST{j}=(MNIST{j}-mean_selected_images*ones(1,size(MNIST{j},2)))/sqrt(norm2_selected_images)*sqrt(p);
<span class="keyword">end</span>

means = @(i) mean(MNIST{i},2);
MNIST_n = @(i) size(MNIST{i},2);
emp_covs = @(i) 1/MNIST_n(i)*(MNIST{i}*MNIST{i}')-means(i)*means(i)';

nb_average_loop = 30;

tau_estim = 0;
store_output = zeros(nb_average_loop,n_test);
<span class="keyword">for</span> average_index = 1:nb_average_loop
    X=zeros(p,n);
    X_test=zeros(p,n_test);
    <span class="keyword">for</span> i=1:k
        data = MNIST{i}(:,randperm(size(MNIST{i},2)));
        X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
        X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
    <span class="keyword">end</span>
    XX = X'*X/p;
    tau_estim = tau_estim + 2/n*trace(XX)/nb_average_loop;

    K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');

    y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
    y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];

    inv_Q = K+n/gamma*eye(n);
    Q_y = inv_Q\y;
    Q_1 = inv_Q\ones(n,1);

    b = sum(Q_y)/sum(Q_1);
    alpha = Q_y-Q_1*b;

    g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;

    store_output(average_index,:) = g(X_test);
<span class="keyword">end</span>

<span class="comment">% theory</span>
Delta_means = means(1) - means(2);
Delta_covs = emp_covs(1) - emp_covs(2);
trace_delta_covs2 = trace(emp_covs(1)*emp_covs(1) + emp_covs(2)*emp_covs(2)) - trace(emp_covs(1))^2/MNIST_n(1) - trace(emp_covs(1))^2/MNIST_n(1) -2*trace(emp_covs(1)*emp_covs(2));
derivs = [f(tau_estim) -f(tau_estim)/2 f(tau_estim)/4];

<span class="comment">%D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace(Delta_covs*Delta_covs) )/p;</span>
D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace_delta_covs2 )/p;
E = @(a) cs(2) - cs(1) + 2*(-1)^a*(1-cs(a))*gamma*cs(1)*cs(2)*D/p;
V = @(a) 8*gamma^2*cs(1)^2*cs(2)^2*( derivs(3)^2*(trace(Delta_covs))^2*(trace(emp_covs(a)*emp_covs(a)) - trace(emp_covs(a))^2/MNIST_n(a) )/p/p + 2*derivs(2)^2*( Delta_means'*emp_covs(a)*Delta_means + trace( emp_covs(a)*( emp_covs(1)/cs(1) + emp_covs(2)/cs(2) ) )/n - trace( emp_covs(a))^2/n/n ) )/p/p;

xs = linspace(min(store_output(:)), max(store_output(:)), 200);

figure
hold <span class="string">on</span>
histogram(reshape(store_output(:,1:cs(1)*n_test),nb_average_loop*cs(1)*n_test,1), 30, <span class="string">'Normalization'</span>, <span class="string">'pdf'</span>)
histogram(reshape(store_output(:,cs(1)*n_test+1:end),nb_average_loop*cs(2)*n_test,1), 30, <span class="string">'Normalization'</span>, <span class="string">'pdf'</span>)
plot(xs,normpdf(xs,E(1),sqrt(V(1))),<span class="string">'b--'</span>,<span class="string">'LineWidth'</span>,2);
plot(xs,normpdf(xs,E(2),sqrt(V(2))),<span class="string">'r--'</span>,<span class="string">'LineWidth'</span>,2);
ylabel(<span class="string">'Histogram of $g(x)$'</span>, <span class="string">'Interpreter'</span>, <span class="string">'latex'</span>)
</pre><img vspace="5" hspace="5" src="kernel_ridge_02.png" alt=""> <h2 id="3">Misclassification of real data as a function of decision threshold <img src="kernel_ridge_eq07512430879093679749.png" alt="$\xi$" style="width:3px;height:6px;"></h2><pre class="codeinput">close <span class="string">all</span>; clear; clc

cs = [7/16 9/16];
k = length(cs); <span class="comment">% nb of classes</span>
n = 512;
n_test = 128;

gamma = 1; <span class="comment">% regularization</span>
f = @(t) exp(-t/2); <span class="comment">% RBF kernel</span>

xi_loop = 0:0.005:0.25;
store_error = zeros(length(xi_loop),2);

data_choice = <span class="string">'MNIST'</span>; <span class="comment">% MNIST or Fashion-MNIST</span>

<span class="keyword">switch</span> data_choice
    <span class="keyword">case</span> <span class="string">'MNIST'</span>
        selected_labels=[7 9]; <span class="comment">% mean [0 1], [5 6]</span>
        init_data = loadMNISTImages(<span class="string">'../datasets/MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'fashion'</span>
        selected_labels=[8 9];
        init_data = loadMNISTImages(<span class="string">'../datasets/fashion-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/fashion-MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'Kuzushiji'</span>
        selected_labels=[3 4];
        init_data = loadMNISTImages(<span class="string">'../datasets/Kuzushiji-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/Kuzushiji-MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'kannada'</span>
        selected_labels=[4 8];
        init_data = loadMNISTImages(<span class="string">'../datasets/kannada-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'../datasets/kannada-MNIST/train-labels-idx1-ubyte'</span>);
<span class="keyword">end</span>

[labels,idx_init_labels]=sort(init_labels,<span class="string">'ascend'</span>);
images=init_data(:,idx_init_labels);
init_n=length(images(1,:));

p=length(images(:,1));

mean_images=mean(images,2);
norm2_images=0;
<span class="keyword">for</span> i=1:init_n
    norm2_images=norm2_images+1/init_n*norm(images(:,i)-mean_images)^2;
<span class="keyword">end</span>
images=(images-mean_images*ones(1,size(images,2)))/sqrt(norm2_images)*sqrt(p);


selected_images=[];
MNIST = cell(length(selected_labels),1);
j=1;
<span class="keyword">for</span> i=selected_labels
    selected_images=[selected_images images(:,labels==i)];
    MNIST{j}=images(:,labels==i);
    j=j+1;
<span class="keyword">end</span>

mean_selected_images=mean(selected_images,2);
norm2_selected_images=mean(sum(abs(selected_images-mean_selected_images*ones(1,length(selected_images))).^2));

<span class="keyword">for</span> j=1:length(selected_labels)
    MNIST{j}=(MNIST{j}-mean_selected_images*ones(1,size(MNIST{j},2)))/sqrt(norm2_selected_images)*sqrt(p);
<span class="keyword">end</span>


iter = 1;
<span class="keyword">for</span> xi = xi_loop

    nb_average_loop = 30;
    store_output = zeros(nb_average_loop,1);
    <span class="keyword">for</span>  average_index = 1:nb_average_loop
        X=zeros(p,n);
        X_test=zeros(p,n_test);
        <span class="keyword">for</span> i=1:k
            data = MNIST{i}(:,randperm(size(MNIST{i},2)));
            X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
            X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
        <span class="keyword">end</span>
        XX = X'*X/p;

        K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');

        y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
        y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];

        inv_Q = K+n/gamma*eye(n);
        Q_y = inv_Q\y;
        Q_1 = inv_Q\ones(n,1);

        b = sum(Q_y)/sum(Q_1);
        alpha = Q_y-Q_1*b;

        g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;

        g_test = g(X_test)';
        store_output(average_index) = 1 - 1/n_test*(sum(g_test(1:cs(1)*n_test)&lt;xi)+sum(g_test(cs(1)*n_test+1:end)&gt;xi ));
    <span class="keyword">end</span>

    store_error(iter,:) = [mean(store_output), std(store_output)];
    iter = iter + 1;
<span class="keyword">end</span>

figure
hold <span class="string">on</span>
errorbar(xi_loop, store_error(:,1), store_error(:,2))
xline(cs(2) - cs(1),<span class="string">'--m'</span>);
xlabel(<span class="string">'Decision threshold $\xi$'</span>, <span class="string">'Interpreter'</span>, <span class="string">'latex'</span>)
ylabel(<span class="string">'Misclassification rate'</span>, <span class="string">'Interpreter'</span>, <span class="string">'latex'</span>)
</pre><img vspace="5" hspace="5" src="kernel_ridge_03.png" alt=""> <p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2020a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Section 4.5.3 Kernel ridge regression
% This page contains simulations in Section 4.5.3.

%% Classification of two mixture with same mean and covariance trace
close all; clear; clc

cs = [1/2 1/2];
k = length(cs); % number of classes
p = 512;
n = 2048;
n_test = 512;

means = @(i) zeros(p,1);
covs = @(i) toeplitz((4*(i-1)/10).^(0:(p-1)));

Delta_means = means(1)-means(2);
Delta_covs = covs(1) - covs(2);

gamma = 1;
tau = 2*trace(cs(1)*covs(1) + cs(2)*covs(2))/p;

fp_tau_loop = -3:.5:3;
store_error = zeros(length(fp_tau_loop),3);

iter = 1;
for fp_tau = fp_tau_loop
    
    derivs=[4 fp_tau 2];
    coeffs=zeros(1,length(derivs));
    for i=1:length(derivs)
        coeffs(i)=derivs(length(derivs)+1-i)/factorial(length(derivs)-i);
    end
    f = @(x) polyval(coeffs,x-tau);
    
    nb_average_loop = 30;
    store_output = zeros(nb_average_loop,1);
    for  average_index = 1:nb_average_loop
        W=zeros(p,n);
        W_test=zeros(p,n_test);
        for i=1:k
            W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=covs(i)^(1/2)*randn(p,cs(i)*n);
            W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=covs(i)^(1/2)*randn(p,cs(i)*n_test);
        end
        
        X=zeros(p,n);
        X_test=zeros(p,n_test);
        for i=1:k
            X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)+means(i)*ones(1,cs(i)*n);
            X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)+means(i)*ones(1,cs(i)*n_test);
        end
        XX = X'*X/p;
        
        K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');
        
        y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
        y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];
        
        inv_Q = K+n/gamma*eye(n);
        Q_y = inv_Q\y;
        Q_1 = inv_Q\ones(n,1);
        
        b = sum(Q_y)/sum(Q_1);
        alpha = Q_y-Q_1*b;
        
        g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;
        
        g_test = g(X_test)';
        store_output(average_index) = 1 - 1/n_test*(sum(g_test(1:cs(1)*n_test)<0)+sum(g_test(cs(1)*n_test+1:end)>0 ));
    end
    
    % theory
    D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace(Delta_covs*Delta_covs) )/p;
    E = @(a) cs(2) - cs(1) + 2*(-1)^a*(1-cs(a))*gamma*cs(1)*cs(2)*D/p;
    V = @(a) 8*gamma^2*cs(1)^2*cs(2)^2*( derivs(3)^2*(trace(Delta_covs))^2*trace(covs(a)*covs(a))/p/p + 2*derivs(2)^2*( Delta_means'*covs(a)*Delta_means + trace( covs(a)*( covs(1)/cs(1) + covs(2)/cs(2) ) )/n ) )/p/p;
    
    store_error(iter,:) = [mean(store_output), std(store_output), cs(1)*qfunc(-E(1)/sqrt(V(1))) + cs(2)*qfunc(E(2)/sqrt(V(2)) )];
    iter = iter + 1;
end

figure
hold on
errorbar(fp_tau_loop, store_error(:,1), store_error(:,2))
plot(fp_tau_loop, store_error(:,3), 'r')
xlabel( '$f^\prime(\tau_p)$', 'Interpreter', 'latex')
ylabel('Misclassification rate', 'Interpreter', 'latex')

%% Normal approximation of soft decision function $g(x)$
close all; clear; clc

cs = [1/2 1/2];
k = length(cs); % nb of classes
n = 2048;
n_test = 512;

gamma = 1; % regularization
f = @(t) exp(-t/2); % RBF kernel

data_choice = 'MNIST'; % 'MNIST', 'fashion',

switch data_choice
    case 'MNIST'
        selected_labels=[7 9]; % mean [0 1], [5 6]
        init_data = loadMNISTImages('../datasets/MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/MNIST/train-labels-idx1-ubyte');
    case 'fashion'
        selected_labels=[8 9];
        init_data = loadMNISTImages('../datasets/fashion-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/fashion-MNIST/train-labels-idx1-ubyte');
    case 'Kuzushiji'
        selected_labels=[3 4];
        init_data = loadMNISTImages('../datasets/Kuzushiji-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/Kuzushiji-MNIST/train-labels-idx1-ubyte');
    case 'kannada'
        selected_labels=[4 8];
        init_data = loadMNISTImages('../datasets/kannada-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/kannada-MNIST/train-labels-idx1-ubyte');
end


[labels,idx_init_labels]=sort(init_labels,'ascend');
images=init_data(:,idx_init_labels);
init_n=length(images(1,:));

p=length(images(:,1));

mean_images=mean(images,2);
norm2_images=0;
for i=1:init_n
    norm2_images=norm2_images+1/init_n*norm(images(:,i)-mean_images)^2;
end
images=(images-mean_images*ones(1,size(images,2)))/sqrt(norm2_images)*sqrt(p);


selected_images=[];
MNIST = cell(length(selected_labels),1);
j=1;
for i=selected_labels
    selected_images=[selected_images images(:,labels==i)];
    MNIST{j}=images(:,labels==i);
    j=j+1;
end

mean_selected_images=mean(selected_images,2);
norm2_selected_images=mean(sum(abs(selected_images-mean_selected_images*ones(1,length(selected_images))).^2));

for j=1:length(selected_labels)
    MNIST{j}=(MNIST{j}-mean_selected_images*ones(1,size(MNIST{j},2)))/sqrt(norm2_selected_images)*sqrt(p);
end

means = @(i) mean(MNIST{i},2);
MNIST_n = @(i) size(MNIST{i},2);
emp_covs = @(i) 1/MNIST_n(i)*(MNIST{i}*MNIST{i}')-means(i)*means(i)';

nb_average_loop = 30;

tau_estim = 0;
store_output = zeros(nb_average_loop,n_test);
for average_index = 1:nb_average_loop
    X=zeros(p,n);
    X_test=zeros(p,n_test);
    for i=1:k
        data = MNIST{i}(:,randperm(size(MNIST{i},2)));
        X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
        X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
    end
    XX = X'*X/p;
    tau_estim = tau_estim + 2/n*trace(XX)/nb_average_loop;
    
    K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');
    
    y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
    y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];
    
    inv_Q = K+n/gamma*eye(n);
    Q_y = inv_Q\y;
    Q_1 = inv_Q\ones(n,1);
    
    b = sum(Q_y)/sum(Q_1);
    alpha = Q_y-Q_1*b;
    
    g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;
    
    store_output(average_index,:) = g(X_test);
end

% theory
Delta_means = means(1) - means(2);
Delta_covs = emp_covs(1) - emp_covs(2);
trace_delta_covs2 = trace(emp_covs(1)*emp_covs(1) + emp_covs(2)*emp_covs(2)) - trace(emp_covs(1))^2/MNIST_n(1) - trace(emp_covs(1))^2/MNIST_n(1) -2*trace(emp_covs(1)*emp_covs(2));
derivs = [f(tau_estim) -f(tau_estim)/2 f(tau_estim)/4];

%D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace(Delta_covs*Delta_covs) )/p;
D = -2*derivs(2)*norm(Delta_means)^2 + derivs(3)*( trace(Delta_covs)^2 + 2*trace_delta_covs2 )/p;
E = @(a) cs(2) - cs(1) + 2*(-1)^a*(1-cs(a))*gamma*cs(1)*cs(2)*D/p;
V = @(a) 8*gamma^2*cs(1)^2*cs(2)^2*( derivs(3)^2*(trace(Delta_covs))^2*(trace(emp_covs(a)*emp_covs(a)) - trace(emp_covs(a))^2/MNIST_n(a) )/p/p + 2*derivs(2)^2*( Delta_means'*emp_covs(a)*Delta_means + trace( emp_covs(a)*( emp_covs(1)/cs(1) + emp_covs(2)/cs(2) ) )/n - trace( emp_covs(a))^2/n/n ) )/p/p;

xs = linspace(min(store_output(:)), max(store_output(:)), 200);

figure
hold on
histogram(reshape(store_output(:,1:cs(1)*n_test),nb_average_loop*cs(1)*n_test,1), 30, 'Normalization', 'pdf')
histogram(reshape(store_output(:,cs(1)*n_test+1:end),nb_average_loop*cs(2)*n_test,1), 30, 'Normalization', 'pdf')
plot(xs,normpdf(xs,E(1),sqrt(V(1))),'bREPLACE_WITH_DASH_DASH','LineWidth',2);
plot(xs,normpdf(xs,E(2),sqrt(V(2))),'rREPLACE_WITH_DASH_DASH','LineWidth',2);
ylabel('Histogram of $g(x)$', 'Interpreter', 'latex')


%% Misclassification of real data as a function of decision threshold $\xi$
close all; clear; clc

cs = [7/16 9/16];
k = length(cs); % nb of classes
n = 512;
n_test = 128;

gamma = 1; % regularization
f = @(t) exp(-t/2); % RBF kernel

xi_loop = 0:0.005:0.25;
store_error = zeros(length(xi_loop),2);

data_choice = 'MNIST'; % MNIST or Fashion-MNIST

switch data_choice
    case 'MNIST'
        selected_labels=[7 9]; % mean [0 1], [5 6]
        init_data = loadMNISTImages('../datasets/MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/MNIST/train-labels-idx1-ubyte');
    case 'fashion'
        selected_labels=[8 9];
        init_data = loadMNISTImages('../datasets/fashion-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/fashion-MNIST/train-labels-idx1-ubyte');
    case 'Kuzushiji'
        selected_labels=[3 4];
        init_data = loadMNISTImages('../datasets/Kuzushiji-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/Kuzushiji-MNIST/train-labels-idx1-ubyte');
    case 'kannada'
        selected_labels=[4 8];
        init_data = loadMNISTImages('../datasets/kannada-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('../datasets/kannada-MNIST/train-labels-idx1-ubyte');
end

[labels,idx_init_labels]=sort(init_labels,'ascend');
images=init_data(:,idx_init_labels);
init_n=length(images(1,:));

p=length(images(:,1));

mean_images=mean(images,2);
norm2_images=0;
for i=1:init_n
    norm2_images=norm2_images+1/init_n*norm(images(:,i)-mean_images)^2;
end
images=(images-mean_images*ones(1,size(images,2)))/sqrt(norm2_images)*sqrt(p);


selected_images=[];
MNIST = cell(length(selected_labels),1);
j=1;
for i=selected_labels
    selected_images=[selected_images images(:,labels==i)];
    MNIST{j}=images(:,labels==i);
    j=j+1;
end

mean_selected_images=mean(selected_images,2);
norm2_selected_images=mean(sum(abs(selected_images-mean_selected_images*ones(1,length(selected_images))).^2));

for j=1:length(selected_labels)
    MNIST{j}=(MNIST{j}-mean_selected_images*ones(1,size(MNIST{j},2)))/sqrt(norm2_selected_images)*sqrt(p);
end


iter = 1;
for xi = xi_loop
    
    nb_average_loop = 30;
    store_output = zeros(nb_average_loop,1);
    for  average_index = 1:nb_average_loop
        X=zeros(p,n);
        X_test=zeros(p,n_test);
        for i=1:k
            data = MNIST{i}(:,randperm(size(MNIST{i},2)));
            X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
            X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
        end
        XX = X'*X/p;
        
        K = f(-2*(XX)+diag(XX)*ones(1,n)+ones(n,1)*diag(XX)');
        
        y = [-ones(cs(1)*n,1);ones(cs(2)*n,1)];
        y_test = [-ones(cs(1)*n_test,1);ones(cs(2)*n_test,1)];
        
        inv_Q = K+n/gamma*eye(n);
        Q_y = inv_Q\y;
        Q_1 = inv_Q\ones(n,1);
        
        b = sum(Q_y)/sum(Q_1);
        alpha = Q_y-Q_1*b;
        
        g = @(Y) alpha'*f(diag(XX)*ones(1,size(Y,2))+ones(n,1)*diag(Y'*Y/p)'-2*(X'*Y/p))+b;
        
        g_test = g(X_test)';
        store_output(average_index) = 1 - 1/n_test*(sum(g_test(1:cs(1)*n_test)<xi)+sum(g_test(cs(1)*n_test+1:end)>xi ));
    end
    
    store_error(iter,:) = [mean(store_output), std(store_output)];
    iter = iter + 1;
end

figure
hold on
errorbar(xi_loop, store_error(:,1), store_error(:,2))
xline(cs(2) - cs(1),'REPLACE_WITH_DASH_DASHm');
xlabel('Decision threshold $\xi$', 'Interpreter', 'latex')
ylabel('Misclassification rate', 'Interpreter', 'latex')

##### SOURCE END #####
--></body></html>