
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Section 3.1.2: Linear discriminant analysis (LDA)</title><meta name="generator" content="MATLAB 9.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2020-07-04"><meta name="DC.source" content="LDA.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Section 3.1.2: Linear discriminant analysis (LDA)</h1><!--introduction--><p>This page contains simulations in Section 3.1.2 Hypotheses testing between two Gaussian models: <img src="LDA_eq10009687358208409966.png" alt="$\mathcal N(\mu_0, C_0)$"> versus <img src="LDA_eq04133729760021769134.png" alt="$\mathcal N(\mu_1, C_1)$"></p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Basic setting</a></li><li><a href="#2">Empirical evaluation of LDA</a></li><li><a href="#3">Theoretical predictions of LDA decision (soft) output</a></li></ul></div><h2 id="1">Basic setting</h2><pre class="codeinput">close <span class="string">all</span>; clear; clc

testcase = <span class="string">'kannada-MNIST'</span>; <span class="comment">% Among 'GMM', 'MNIST' and 'fashion-MNIST', 'kannada-MNIST'</span>
testcase_option = <span class="string">'mixed'</span>; <span class="comment">% when testcase = 'GMM', among 'mean', 'cov', 'orth' and 'mixed'</span>

coeff = 1;
n = 2048*coeff;
n_test = 128*coeff;
cs = [1/2 1/2];
k = length(cs);
rng(928);

<span class="keyword">switch</span> testcase
    <span class="keyword">case</span> <span class="string">'GMM'</span>
        p = 512*coeff;
        <span class="keyword">switch</span> testcase_option <span class="comment">% l = 0 or 1</span>
            <span class="keyword">case</span> <span class="string">'means'</span>
                means = @(l) [zeros(l+1,1);1;zeros(p-l-2,1)]*3;
                <span class="comment">%means = @(l) sqrt(2)*(-1)^l*[ones(p/2,1); -ones(p/2,1)]/sqrt(p);</span>
                covs  = @(l) eye(p);
            <span class="keyword">case</span> <span class="string">'cov'</span>
                means = @(l) zeros(p,1);
                covs  = @(l) eye(p)*(1+l/sqrt(p)*15);
            <span class="keyword">case</span> <span class="string">'orth'</span>
                means = @(l) zeros(p,1);
                covs = @(l) toeplitz((4*l/10).^(0:(p-1)));
            <span class="keyword">case</span> <span class="string">'mixed'</span>
                means = @(l) [zeros(l+1,1);1;zeros(p-l-2,1)]*3;
                covs = @(l) toeplitz((4*l/10).^(0:(p-1)));
                <span class="comment">%covs  = @(l) eye(p)*(1+l/sqrt(p)*20);</span>
                <span class="comment">%covs = @(l) toeplitz((4*l/10).^(0:(p-1)))*(1+l/sqrt(p)*4);</span>
        <span class="keyword">end</span>

    <span class="keyword">case</span> <span class="string">'MNIST'</span>
        init_data = loadMNISTImages(<span class="string">'./../../datasets/MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'./../../datasets/MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'fashion-MNIST'</span>
        init_data = loadMNISTImages(<span class="string">'./../../datasets/fashion-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'./../../datasets/fashion-MNIST/train-labels-idx1-ubyte'</span>);
    <span class="keyword">case</span> <span class="string">'kannada-MNIST'</span>
        init_data = loadMNISTImages(<span class="string">'./../../datasets/kannada-MNIST/train-images-idx3-ubyte'</span>);
        init_labels = loadMNISTLabels(<span class="string">'./../../datasets/kannada-MNIST/train-labels-idx1-ubyte'</span>);
<span class="keyword">end</span>

<span class="keyword">switch</span> testcase <span class="comment">% real-world data pre-processing</span>
    <span class="keyword">case</span> {<span class="string">'MNIST'</span>, <span class="string">'fashion-MNIST'</span>,<span class="string">'kannada-MNIST'</span>}
        [labels,idx_init_labels]=sort(init_labels,<span class="string">'ascend'</span>);
        data=init_data(:,idx_init_labels);

        init_n=length(data(1,:));
        p=length(data(:,1));

        selected_labels=[3 4];

        data = data/max(data(:));
        mean_data=mean(data,2);
        norm2_data=0;
        <span class="keyword">for</span> i=1:init_n
            norm2_data=norm2_data+1/init_n*norm(data(:,i)-mean_data)^2;
        <span class="keyword">end</span>
        data=(data-mean_data*ones(1,size(data,2)))/sqrt(norm2_data)*sqrt(p);

        selected_data = cell(k,1);
        cascade_selected_data=[];
        j=1;
        <span class="keyword">for</span> i=selected_labels
            selected_data{j}=data(:,labels==i);
            cascade_selected_data = [cascade_selected_data, selected_data{j}];
            j = j+1;
        <span class="keyword">end</span>

        <span class="comment">% recentering of the k classes</span>
        mean_selected_data=mean(cascade_selected_data,2);
        norm2_selected_data=mean(sum(abs(cascade_selected_data-mean_selected_data*ones(1,size(cascade_selected_data,2))).^2));

        <span class="keyword">for</span> j=1:length(selected_labels)
            selected_data{j}=(selected_data{j}-mean_selected_data*ones(1,size(selected_data{j},2)))/sqrt(norm2_selected_data)*sqrt(p);
        <span class="keyword">end</span>

        means = @(l) mean(selected_data{l+1},2);
        covs = @(l) 1/length(selected_data{l+1})*(selected_data{l+1}*selected_data{l+1}')-means(l)*means(l)';
<span class="keyword">end</span>
</pre><h2 id="2">Empirical evaluation of LDA</h2><pre class="codeinput">gamma = .1; <span class="comment">% regularization parameter</span>

nb_loop = 30;
T_store = zeros(n_test,nb_loop);
accuracy_store = zeros(nb_loop,1);
<span class="keyword">for</span> data_loop = 1:nb_loop

    <span class="keyword">switch</span> testcase <span class="comment">% generate data</span>
        <span class="keyword">case</span> {<span class="string">'MNIST'</span>, <span class="string">'fashion-MNIST'</span>,<span class="string">'kannada-MNIST'</span>}
            X=zeros(p,n);
            X_test=zeros(p,n_test);
            <span class="keyword">for</span> i=1:k <span class="comment">% random data picking</span>
                data = selected_data{i}(:,randperm(size(selected_data{i},2)));
                X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
                X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
            <span class="keyword">end</span>
        <span class="keyword">case</span> <span class="string">'GMM'</span>
            W=zeros(p,n);
            W_test=zeros(p,n_test);
            <span class="keyword">for</span> i=1:k
                W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=sqrtm(covs(i-1))*randn(p,cs(i)*n);
                W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=sqrtm(covs(i-1))*randn(p,cs(i)*n_test);
            <span class="keyword">end</span>

            X=zeros(p,n);
            X_test=zeros(p,n_test);
            <span class="keyword">for</span> i=1:k
                X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)+means(i-1)*ones(1,cs(i)*n);
                X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)+means(i-1)*ones(1,cs(i)*n_test);
            <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% run LDA</span>
    X_train0 = X(:,1:n*cs(1));
    X_train1 = X(:,n*cs(1)+1:end);

    hat_mu0 = X_train0*ones(n*cs(1),1)/(n*cs(1));
    hat_mu1 = X_train1*ones(n*cs(2),1)/(n*cs(2));
    hat_mu = (hat_mu0 + hat_mu1)/2;

    P = @(l) eye(cs(l+1)*n) - ones(cs(l+1)*n, cs(l+1)*n)/(cs(l+1)*n);
    hat_C_gamma = ( X_train0*P(0)*(X_train0') + X_train1*P(1)*(X_train1') )/(n-2) + gamma*eye(p);

    <span class="comment">% decision function</span>
    T = @(x) (x - hat_mu)'*( hat_C_gamma\(hat_mu0 - hat_mu1) );
    T_store(:,data_loop) = T(X_test);

    accuracy_store(data_loop) = sum(T(X_test(:,1:cs(1)*n_test))&gt;0)/(n_test*cs(1))/2+sum(T(X_test(:,cs(1)*n_test+1:end))&lt;0)/(n_test*cs(2))/2;
<span class="keyword">end</span>

T_store0 = T_store(1:cs(1)*n_test,:);
T_store1 = T_store(cs(1)*n_test+1:end,:);

disp([<span class="string">'Classif accuracy:'</span>, num2str(mean(accuracy_store))]);
</pre><pre class="codeoutput">Classif accuracy:0.9875
</pre><h2 id="3">Theoretical predictions of LDA decision (soft) output</h2><pre class="codeinput">eigs_C = @(l) eig(covs(l));

z = - gamma;
tilde_g = ones(2,1);
tilde_g_tmp = zeros(2,1);
g = zeros(2,1);

<span class="comment">%watch_dog = 1;</span>
<span class="keyword">while</span> min(abs(tilde_g-tilde_g_tmp))&gt;1e-6 <span class="comment">%%&amp;&amp; watch_dog&lt;50</span>
    tilde_g_tmp = tilde_g;

    eigs_C_sum = cs(1)*tilde_g(1)*eigs_C(0) + cs(2)*tilde_g(2)*eigs_C(1);

    <span class="keyword">for</span> a = 1:2
        g(a) = -1/z*sum( eigs_C(a-1)./(1 + eigs_C_sum) )/n;
        tilde_g(a) = -1/z/(1+g(a));
    <span class="keyword">end</span>

<span class="keyword">end</span>
bar_Q = -1/z*inv( eye(p) + cs(1)*tilde_g(1)*covs(0) + cs(2)*tilde_g(2)*covs(1) );

S = gamma^2*[cs(1)*tilde_g(1)^2*trace( covs(0)*bar_Q*covs(0)*bar_Q )/n, cs(2)*tilde_g(1)^2*trace( covs(0)*bar_Q*covs(1)*bar_Q )/n; cs(1)*tilde_g(2)^2*trace( covs(0)*bar_Q*covs(1)*bar_Q )/n, cs(2)*tilde_g(2)^2*trace( covs(1)*bar_Q*covs(1)*bar_Q )/n];
tmp_S = (eye(2) - S)\S;
R = @(ll,l) cs(ll+1)/cs(l+1)*tmp_S(ll+1,l+1);

bar_QCQ = @(l) bar_Q*covs(l)*bar_Q + R(0,l)*bar_Q*covs(0)*bar_Q + R(1,l)*bar_Q*covs(1)*bar_Q;

delta_mu = means(0) - means(1);
theo_mean = @(l) (-1)^l*delta_mu'*bar_Q*delta_mu/2 - g(1)/2 + g(2)/2;
theo_var = @(l) delta_mu'*bar_QCQ(l)*delta_mu + trace(covs(0)*bar_QCQ(l))/(n*cs(1)) + trace(covs(1)*bar_QCQ(l))/(n*cs(2));

edges = linspace(min([T_store0(:);T_store1(:)])-.5,max([T_store0(:);T_store1(:)])+.5,300);

figure
hold <span class="string">on</span>
histogram(T_store0(:),30,<span class="string">'Normalization'</span>,<span class="string">'pdf'</span>);
histogram(T_store1(:),30,<span class="string">'Normalization'</span>,<span class="string">'pdf'</span>);
plot(edges,normpdf(edges, theo_mean(0), sqrt(theo_var(0))),<span class="string">'--b'</span>);
plot(edges,normpdf(edges, theo_mean(1), sqrt(theo_var(1))),<span class="string">'--r'</span>);
legend(<span class="string">'empirical $T(x\sim \mathcal H_0)$'</span>, <span class="string">'empirical $T(x\sim \mathcal H_1)$'</span>, <span class="string">'theory $T(x\sim \mathcal H_0)$'</span>, <span class="string">'theory $T(x\sim \mathcal H_1)$'</span>, <span class="string">'Interpreter'</span>,<span class="string">'latex'</span>, <span class="string">'FontSize'</span>, 15)
</pre><img vspace="5" hspace="5" src="LDA_01.png" alt=""> <p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2019a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Section 3.1.2: Linear discriminant analysis (LDA)
% This page contains simulations in Section 3.1.2
% Hypotheses testing between two Gaussian models: $\mathcal N(\mu_0, C_0)$ versus $\mathcal N(\mu_1, C_1)$

%% Basic setting
close all; clear; clc

testcase = 'kannada-MNIST'; % Among 'GMM', 'MNIST' and 'fashion-MNIST', 'kannada-MNIST'
testcase_option = 'mixed'; % when testcase = 'GMM', among 'mean', 'cov', 'orth' and 'mixed'

coeff = 1;
n = 2048*coeff;
n_test = 128*coeff;
cs = [1/2 1/2]; 
k = length(cs);
rng(928);

switch testcase
    case 'GMM'
        p = 512*coeff; 
        switch testcase_option % l = 0 or 1
            case 'means'
                means = @(l) [zeros(l+1,1);1;zeros(p-l-2,1)]*3;
                %means = @(l) sqrt(2)*(-1)^l*[ones(p/2,1); -ones(p/2,1)]/sqrt(p);
                covs  = @(l) eye(p);
            case 'cov'
                means = @(l) zeros(p,1);
                covs  = @(l) eye(p)*(1+l/sqrt(p)*15);
            case 'orth'
                means = @(l) zeros(p,1);
                covs = @(l) toeplitz((4*l/10).^(0:(p-1)));
            case 'mixed'
                means = @(l) [zeros(l+1,1);1;zeros(p-l-2,1)]*3;
                covs = @(l) toeplitz((4*l/10).^(0:(p-1)));
                %covs  = @(l) eye(p)*(1+l/sqrt(p)*20);
                %covs = @(l) toeplitz((4*l/10).^(0:(p-1)))*(1+l/sqrt(p)*4);
        end
        
    case 'MNIST'
        init_data = loadMNISTImages('./../../datasets/MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('./../../datasets/MNIST/train-labels-idx1-ubyte');
    case 'fashion-MNIST'
        init_data = loadMNISTImages('./../../datasets/fashion-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('./../../datasets/fashion-MNIST/train-labels-idx1-ubyte');
    case 'kannada-MNIST'
        init_data = loadMNISTImages('./../../datasets/kannada-MNIST/train-images-idx3-ubyte');
        init_labels = loadMNISTLabels('./../../datasets/kannada-MNIST/train-labels-idx1-ubyte');
end

switch testcase % real-world data pre-processing 
    case {'MNIST', 'fashion-MNIST','kannada-MNIST'}
        [labels,idx_init_labels]=sort(init_labels,'ascend');
        data=init_data(:,idx_init_labels);
        
        init_n=length(data(1,:));
        p=length(data(:,1));
        
        selected_labels=[3 4];   
        
        data = data/max(data(:));
        mean_data=mean(data,2);
        norm2_data=0;
        for i=1:init_n
            norm2_data=norm2_data+1/init_n*norm(data(:,i)-mean_data)^2;
        end
        data=(data-mean_data*ones(1,size(data,2)))/sqrt(norm2_data)*sqrt(p);
        
        selected_data = cell(k,1);
        cascade_selected_data=[];
        j=1;
        for i=selected_labels
            selected_data{j}=data(:,labels==i);
            cascade_selected_data = [cascade_selected_data, selected_data{j}];
            j = j+1;
        end
        
        % recentering of the k classes
        mean_selected_data=mean(cascade_selected_data,2);
        norm2_selected_data=mean(sum(abs(cascade_selected_data-mean_selected_data*ones(1,size(cascade_selected_data,2))).^2));
        
        for j=1:length(selected_labels)
            selected_data{j}=(selected_data{j}-mean_selected_data*ones(1,size(selected_data{j},2)))/sqrt(norm2_selected_data)*sqrt(p);
        end
        
        means = @(l) mean(selected_data{l+1},2);
        covs = @(l) 1/length(selected_data{l+1})*(selected_data{l+1}*selected_data{l+1}')-means(l)*means(l)';
end

%% Empirical evaluation of LDA
gamma = .1; % regularization parameter

nb_loop = 30;
T_store = zeros(n_test,nb_loop);
accuracy_store = zeros(nb_loop,1);
for data_loop = 1:nb_loop
    
    switch testcase % generate data
        case {'MNIST', 'fashion-MNIST','kannada-MNIST'}
            X=zeros(p,n);
            X_test=zeros(p,n_test);
            for i=1:k % random data picking
                data = selected_data{i}(:,randperm(size(selected_data{i},2)));
                X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=data(:,1:n*cs(i));
                X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=data(:,n+1:n+n_test*cs(i));
            end
        case 'GMM'
            W=zeros(p,n);
            W_test=zeros(p,n_test);
            for i=1:k
                W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=sqrtm(covs(i-1))*randn(p,cs(i)*n);
                W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=sqrtm(covs(i-1))*randn(p,cs(i)*n_test);
            end
            
            X=zeros(p,n);
            X_test=zeros(p,n_test);
            for i=1:k
                X(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)=W(:,sum(cs(1:(i-1)))*n+1:sum(cs(1:i))*n)+means(i-1)*ones(1,cs(i)*n);
                X_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)=W_test(:,sum(cs(1:(i-1)))*n_test+1:sum(cs(1:i))*n_test)+means(i-1)*ones(1,cs(i)*n_test);
            end
    end
    
    % run LDA
    X_train0 = X(:,1:n*cs(1));
    X_train1 = X(:,n*cs(1)+1:end);
    
    hat_mu0 = X_train0*ones(n*cs(1),1)/(n*cs(1));
    hat_mu1 = X_train1*ones(n*cs(2),1)/(n*cs(2));
    hat_mu = (hat_mu0 + hat_mu1)/2;
    
    P = @(l) eye(cs(l+1)*n) - ones(cs(l+1)*n, cs(l+1)*n)/(cs(l+1)*n);
    hat_C_gamma = ( X_train0*P(0)*(X_train0') + X_train1*P(1)*(X_train1') )/(n-2) + gamma*eye(p);
    
    % decision function
    T = @(x) (x - hat_mu)'*( hat_C_gamma\(hat_mu0 - hat_mu1) );
    T_store(:,data_loop) = T(X_test); 
    
    accuracy_store(data_loop) = sum(T(X_test(:,1:cs(1)*n_test))>0)/(n_test*cs(1))/2+sum(T(X_test(:,cs(1)*n_test+1:end))<0)/(n_test*cs(2))/2;
end

T_store0 = T_store(1:cs(1)*n_test,:);
T_store1 = T_store(cs(1)*n_test+1:end,:);

disp(['Classif accuracy:', num2str(mean(accuracy_store))]);

%% Theoretical predictions of LDA decision (soft) output
eigs_C = @(l) eig(covs(l));

z = - gamma;
tilde_g = ones(2,1);
tilde_g_tmp = zeros(2,1);
g = zeros(2,1);

%watch_dog = 1;
while min(abs(tilde_g-tilde_g_tmp))>1e-6 %%&& watch_dog<50
    tilde_g_tmp = tilde_g;
    
    eigs_C_sum = cs(1)*tilde_g(1)*eigs_C(0) + cs(2)*tilde_g(2)*eigs_C(1);
    
    for a = 1:2
        g(a) = -1/z*sum( eigs_C(a-1)./(1 + eigs_C_sum) )/n;
        tilde_g(a) = -1/z/(1+g(a));
    end
    
end
bar_Q = -1/z*inv( eye(p) + cs(1)*tilde_g(1)*covs(0) + cs(2)*tilde_g(2)*covs(1) );

S = gamma^2*[cs(1)*tilde_g(1)^2*trace( covs(0)*bar_Q*covs(0)*bar_Q )/n, cs(2)*tilde_g(1)^2*trace( covs(0)*bar_Q*covs(1)*bar_Q )/n; cs(1)*tilde_g(2)^2*trace( covs(0)*bar_Q*covs(1)*bar_Q )/n, cs(2)*tilde_g(2)^2*trace( covs(1)*bar_Q*covs(1)*bar_Q )/n];
tmp_S = (eye(2) - S)\S;
R = @(ll,l) cs(ll+1)/cs(l+1)*tmp_S(ll+1,l+1);

bar_QCQ = @(l) bar_Q*covs(l)*bar_Q + R(0,l)*bar_Q*covs(0)*bar_Q + R(1,l)*bar_Q*covs(1)*bar_Q;

delta_mu = means(0) - means(1);
theo_mean = @(l) (-1)^l*delta_mu'*bar_Q*delta_mu/2 - g(1)/2 + g(2)/2;
theo_var = @(l) delta_mu'*bar_QCQ(l)*delta_mu + trace(covs(0)*bar_QCQ(l))/(n*cs(1)) + trace(covs(1)*bar_QCQ(l))/(n*cs(2));

edges = linspace(min([T_store0(:);T_store1(:)])-.5,max([T_store0(:);T_store1(:)])+.5,300);

figure
hold on
histogram(T_store0(:),30,'Normalization','pdf');
histogram(T_store1(:),30,'Normalization','pdf');
plot(edges,normpdf(edges, theo_mean(0), sqrt(theo_var(0))),'REPLACE_WITH_DASH_DASHb');
plot(edges,normpdf(edges, theo_mean(1), sqrt(theo_var(1))),'REPLACE_WITH_DASH_DASHr');
legend('empirical $T(x\sim \mathcal H_0)$', 'empirical $T(x\sim \mathcal H_1)$', 'theory $T(x\sim \mathcal H_0)$', 'theory $T(x\sim \mathcal H_1)$', 'Interpreter','latex', 'FontSize', 15)

##### SOURCE END #####
--></body></html>